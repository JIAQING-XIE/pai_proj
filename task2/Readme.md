## Task 2 BNN
As part of this project, we implemented all four techniques: Monte Carlo with dropout, deep ensemble models, stochastic gradient Langevin dynamics (SGLD), and Bayesian neural networks (BNNs). When we validate our model using MC with dropout, we let the networks drop out after activation for each layer and sample dropout models using MC. The given loss is replaced with CrossEntropyLoss even though they are theoretically equivalent. Additionally, we added the weight decay term to our optimizer, as described in the paper as L2 regularization. For the deep ensemble model, five individual models were trained and their prediction probabilities averaged to approximate the true probability. The Adam optimizer has been changed to the SGLD optimizer for the SGLD model, and the model is saved after burn-in as well as at sampling points. Additionally, we changed the batch size to 1024. We found that it is not necessary to use the dropout model under this scheme. As for the BNNs, we implemented the Bayesian Layer first, and defined reasonable priors and posteriors. The bias should be set the same as the weight posterior if the bias term is valid. For the forward function, we set the weights as mu + delta * sigma. We added the likelihood of bias in the prior and posterior separately, and we yield the 3-tuple as mentioned in the documentation. For the Bayes Net Implementation, priors and posteriors are added for each layer. According to the paper, the KL loss term between prior and posterior should be computed with a precise probability pi for each batch. We do not use pi, but rather a simple constant since we assume that all batches have the same probability. We also completed the function of plotting calibration curve as required in the task manual. The most optimal performance was found with MC dropout model. Since we don't spend too much time tuning the hyperparameters, the conclusion might not be accurate. Nevertheless, all of them can pass the baseline test.  