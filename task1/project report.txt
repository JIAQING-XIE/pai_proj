Upon printing the mean and variance of the data, we discovered that the variance was large. Therefore, we scaled the data by standardization, which behaves similarly to the StandardScaler in Sklearn. In the same way, we scaled the labels of the data. These parameters are set during the initialization of the class.  We have trained several local GPs with clusters to include all of the training points. As part of training local GPs, we used KMeans to fit the data to each cluster (each cluster trains an individual GP model). We found that 25 clusters is the optimal. As well as preprocessing the data, we also used the original GP mean and GP standard to make predictions by the following equation: GP mean * y_std (empirical label standard dev) + y_mean (empirical label mean). Having afflined the original data, we do a back-forward affline transformation to let the data return to the original space. It is estimated that the final cost would be approximately 15.18. It was then suggested that torch-based models might be much faster. Our GPytorch model is simple, similar to the example in the GPytorch tutorial. Parameters were tuned but deeper architectures such as Deep Gaussian Process were not added. With the simple model, the job can be completed in one or two minutes, and it also reached the same level as the GPRegressor() in Sklearn. There may be a disadvantage in that the kernel space is too large, so I was unable to find a global optimum. Our group may work on this in the future.